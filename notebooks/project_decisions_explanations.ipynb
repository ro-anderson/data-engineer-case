{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Intro - Patterns Decisions and Implementation Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project a **python library  structure was built** to centralize all the data transformation processes, ensuring consistency, reusability, and maintainability across various data handling tasks.\n",
    "\n",
    "library tree:\n",
    "\n",
    "```bash\n",
    "etl\n",
    "├── factory.py\n",
    "└── __init__.py\n",
    "```\n",
    "\n",
    "This project is full conteinarized so for using it inside these notebook just need to import it - as we are going to do on a next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project adopts a [**medallion architecture**](https://www.databricks.com/glossary/medallion-architecture), processing data through *Bronze*, *Silver*, and *Gold* layers, which emulates a data lake architecture locally. This setup allows for scalable and modular data processing.\n",
    "\n",
    "For this project all the data is saved on .csv or .json files but as the idea is to simulate a Data Lake architecture, we could connect this to a real one and save the files over the data lake (as parquet, for example).\n",
    "\n",
    "We opted for Pandas as the primary data manipulation library for this project due to its efficiency with medium-sized datasets and its rich feature set for data analysis and transformation. Given the current scale of our data, which does not necessitate distributed computing, Pandas offers a good straightforward and developer-friendly environment to achieve our objectives efficiently. \n",
    "\n",
    "Moreover, our ETL system is designed with modularization and employs robust design patterns, which are conducive to scalability. Should the need arise to handle larger datasets or to leverage distributed computing, our architecture allows for a relatively smooth transition to distributed frameworks like Spark, Dask, Polars, or Ray, ensuring future-proofing and adaptability of our data processing capabilities.\n",
    "\n",
    "More in detail about each layer on this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Landing** \n",
    "> This folder acts as the initial ingestion point where raw data is dumped directly from the source systems. No processing or transformation is done at this stage. It’s purely for data intake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bronze** \n",
    "> Data moves from the Landing folder to the Bronze folder after initial logging and perhaps some minor processing like adding metadata (load timestamps, source identifiers, etc.). The Bronze layer should provide capabilities to handle Change Data Capture (CDC), ensure data lineage and auditability, and support reprocessing if necessary without needing to re-read from the source systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silver**\n",
    "> The silver folder represents an intermediate stage of data processing, where data is further cleansed, merged, and conformed into more consistent and usable forms.The silver layer supports more complex data operations that are necessary for deriving meaningful business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gold** \n",
    "> Gold data is the endpoint of the data transformation process. There we have the ouput of the tasks who were designated to this project (tasks 1 to 5). In a real scenario, it represents the highest value in terms of data usability and relevance to business users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline runs through the layers (Bronze, Silver and Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.factory import DataProcessorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(layer, origin_layer_dir, destiny_layer_dir):\n",
    "    \"\"\"\n",
    "    Executes the data processing pipeline for a specified layer using a data processor factory.\n",
    "\n",
    "    This function initializes the data processor for a given layer, sets the directories for data input and output, and triggers the data processing.\n",
    "\n",
    "    Args:\n",
    "        layer (str): The layer of the data processing pipeline to execute. Valid options are 'bronze', 'silver', or 'gold'.\n",
    "        origin_layer_dir (str): The directory where the input data is located. This is the starting point for data processing.\n",
    "        destiny_layer_dir (str): The directory where the processed data should be saved. This acts as the output location for the data.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything but will trigger the processing of data as per the specified layer.\n",
    "    \"\"\"\n",
    "    processor = DataProcessorFactory(origin_layer_dir, destiny_layer_dir)\n",
    "    processor = processor.get_processor(layer)\n",
    "    processor.process_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Landing -> Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-29 23:56:42.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mStarting processing for Bronze layer\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:42.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mReading all files in the landing directory\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:45.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mData saved to Bronze layer at ../data/bronze/prints.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:45.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_json\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mProcessed and saved 508617 records from prints.json to Bronze layer\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:47.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mData saved to Bronze layer at ../data/bronze/pays.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:47.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_csv\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mProcessed and saved 756483 records from pays.csv to Bronze layer\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:47.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mData saved to Bronze layer at ../data/bronze/taps.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:47.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_json\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mProcessed and saved 50859 records from taps.json to Bronze layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "layer = 'bronze'\n",
    "origin_layer_dir = '../data/landing'\n",
    "destiny_layer_dir = '../data/bronze'\n",
    "\n",
    "# Run pipeline\n",
    "pipeline(layer, origin_layer_dir, destiny_layer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bronze -> Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-29 23:56:47.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mProcessing data for Silver layer\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:47.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mReading all files in the bronze directory\u001b[0m\n",
      "Extracting value_prop from event_data: 100%|██████████| 508617/508617 [00:03<00:00, 131406.68it/s]\n",
      "Extracting value_prop from event_data: 100%|██████████| 50859/50859 [00:00<00:00, 129476.50it/s]\n",
      "\u001b[32m2024-04-29 23:56:52.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mProcessed and saved data to Gold layer at ../data/gold/task0_prints_last_3_weeks.json\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:54.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mProcessed and saved to Silver layer at ../data/silver/prints_taps_and_pays_daily.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:54.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mProcessed and saved 411045 records from prints_taps_and_pays_daily.csv to Silver layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "layer = 'silver'\n",
    "origin_layer_dir = '../data/bronze'\n",
    "destiny_layer_dir = '../data/silver'\n",
    "\n",
    "# Run pipeline\n",
    "pipeline(layer, origin_layer_dir, destiny_layer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Silver -> Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-29 23:56:54.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mProcessing data for Gold layer\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:54.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mread_data\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mReading all files in the Silver directory\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:54.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mData read successfully. Ready for further processing.\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:54.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_task1_json\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mProcessed and saved data to Gold layer at ../data/gold/task1_prints_with_clicked_parameter.json\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:56.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m276\u001b[0m - \u001b[1mProcessed and saved data to Gold layer at ../data/gold/task_2_views_on_each_value_prop_last_3_weeks.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:57.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m276\u001b[0m - \u001b[1mProcessed and saved data to Gold layer at ../data/gold/task_3_clickes_on_each_value_prop_last_3_weeks.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:58.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36msave_data\u001b[0m:\u001b[36m276\u001b[0m - \u001b[1mProcessed and saved data to Gold layer at ../data/gold/tasks_4_and_5_user_payments_summary_last_3_weeks.csv\u001b[0m\n",
      "\u001b[32m2024-04-29 23:56:58.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36metl.factory\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mProcessed and saved all data to Golden layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Silver\n",
    "layer = 'gold'\n",
    "origin_layer_dir = '../data/silver'\n",
    "destiny_layer_dir = '../data/gold'\n",
    "\n",
    "# Run pipeline\n",
    "pipeline(layer, origin_layer_dir, destiny_layer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expected Results and Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tasks Definition\n",
    "- **task_0**: *Prints from the Last Week*:\n",
    "- *For each print*:\n",
    "    - **task_1**: A field indicating if the value props were clicked or not.\n",
    "    - **task_2**: The number of views each value proposition received in the last 3 weeks prior to the print.\n",
    "    - **task_3**: The number of times a user clicked on each of the value props in the last 3 weeks prior to the print.\n",
    "    - **task_4**: The number of payments made by the user for each value proposition in the last 3 weeks prior to the print.\n",
    "    - **task_5**: The total amount of payments made by the user for each value proposition in the last 3 weeks prior to the print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first import some utils to print the results:\n",
    "from utils import read_task0_data, read_task1_data, read_task2_data, read_task3_data, read_tasks4_and_5_data\n",
    "\n",
    "json_task0 = read_task0_data()\n",
    "json_task1 = read_task1_data()\n",
    "df_task2 = read_task2_data()\n",
    "df_task3 = read_task3_data()\n",
    "df_task4_and_task5 = read_tasks4_and_5_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **important** \n",
    "> All the code was implemented mainly on the [factory.py](../etl/factory.py), so the idea is to show the final result and a brief explanation about the decisions made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task_0\n",
    "Above is a one line of the [**task0_prints_last_3_weeks.json**](../data/gold/task0_prints_last_3_weeks.json) file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first record on the ../data/gold/task0_prints_last_3_weeks.json:\n",
      "\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"send_money\"},\"user_id\":59706}\n",
      "\n",
      "Some other data inside the ../data/gold/task0_prints_last_3_weeks.json:\n",
      "\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"link_cobro\"},\"user_id\":32191}\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"transport\"},\"user_id\":32191}\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"send_money\"},\"user_id\":32191}\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"prepaid\"},\"user_id\":53960}\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"link_cobro\"},\"user_id\":53960}\n",
      "{\"day\":\"2020-11-30\",\"event_data\":{\"value_prop\":\"send_money\"},\"user_id\":53960}\n"
     ]
    }
   ],
   "source": [
    "# 1 observation of the prints from the last 3 weeks before the last week:\n",
    "# here we have a json file with all the prints from the last_3 weeks.\n",
    "\n",
    "# The first one on the [**task0_prints_last_3_weeks.json**](../data/gold/task0_prints_last_3_weeks.json but you could check the file output for more examples:\n",
    "print(f\"The first record on the ../data/gold/task0_prints_last_3_weeks.json:\\n\\n{json_task0[:78]}\")\n",
    "\n",
    "\n",
    "# As we can see, we just have data on the last week of the 11-2020 month:\n",
    "print(f\"Some other data inside the ../data/gold/task0_prints_last_3_weeks.json:\\n\\n{json_task0[78:541]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to create the prints from the last 3 weeks before the last week we take the steps:\n",
    "1. Extract value_prop from event_data\n",
    "2. define 'week' column indicating the week of the month (based on [series.dt.isocalendar().week](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.isocalendar.html) method)\n",
    "```python\n",
    "               # Extract value_prop from event_data\n",
    "            df_w_prints = self.extract_event_data(data_frames['prints'])\n",
    "            df_w_taps = self.extract_event_data(data_frames['taps'])\n",
    "            df_w_pays = data_frames['pays'].copy()\n",
    "\n",
    "            # Add 'week' column indicating the week of the month\n",
    "            df_w_prints['week'] = df_w_prints['day'].dt.isocalendar().week\n",
    "```\n",
    "after that, we got that, the last week was the number 49 - so, we could esally extract the last week data just filtering it from the data transformation that we was doing at the Silver layer:\n",
    "```python\n",
    "            # task 0:\n",
    "            # Step 1: Filter data for week 49\n",
    "            df_week_49 = df_w_prints[df_w_prints['week'] == 49]\n",
    "            \n",
    "            # Step 2: Transform the DataFrame to match the JSON structure\n",
    "            json_data = df_week_49.apply(self._transform_row_to_prints_format, axis=1).to_json(orient='records', lines=True)\n",
    "\n",
    "```\n",
    "\n",
    "the self.transform_row_to_prints_format is a function to recreate the prints.json struct:\n",
    "```python\n",
    "    def _transform_row_to_prints_format(self, row):\n",
    "        return {\n",
    "            \"day\": row['day'].strftime('%Y-%m-%d'),  # Format day as string if it's a datetime object\n",
    "            \"event_data\": {\n",
    "                \"value_prop\": row['value_prop']\n",
    "            },\n",
    "            \"user_id\": row['user_id']\n",
    "        }\n",
    "```\n",
    "\n",
    "3. Save the data on the gold layer - I choosed to save on the gold layear all the tasks outputs, even this is not actually a \"ready to use\" data.\n",
    "   ```python\n",
    "            with open(output_path, 'w') as file:\n",
    "                file.write(json_data)\n",
    "\n",
    "            logger.info(f\"Processed and saved data to Gold layer at {output_path}\")\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task_1\n",
    "Above is a one line of the [**task1_prints_with_clicked_parameter.json**](../data/gold/task1_prints_with_clicked_parameter.json) file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"user_id\":96544,\"day_prints\":\"2020-11-09\",\"value_prop\":\"point\",\"clicked\":0}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 observation of the prints with clicked paramter\n",
    "json_task1[:77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"user_id\":96544,\"day_prints\":\"2020-11-09\",\"value_prop\":\"point\",\"clicked\":0}\\n{\"user_id\":96544,\"day_prints\":\"2020-11-09\",\"value_prop\":\"cellphone_rechar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 observations...\n",
    "json_task1[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea was to build a similar struct as the [prints.json](../data/landing/prints.json) file:\n",
    "\n",
    "```json\n",
    "{\"day\":\"2020-11-01\",\"event_data\":{\"position\":0,\"value_prop\":\"cellphone_recharge\"},\"user_id\":98702}\n",
    "```\n",
    "\n",
    "the output, example:\n",
    "\n",
    "```json\n",
    "'{\"user_id\":96544,\"day_prints\":\"2020-11-09\",\"value_prop\":\"point\",\"clicked\":0}\\n'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to create the clicked label that could be 0: not clicked or 1: clicked I did the followings decisions:\n",
    "1. Extract value_prop from event_data\n",
    "2. dd 'week' column indicating the week of the month (based on series.dt.isocalendar().week function)\n",
    "3. Define what is a click:\n",
    "    i. day+value_prop on the taps.json and prints.json if is it equal on both files, thas is a click!\n",
    "\n",
    "    ```python\n",
    "                # Extract value_prop from event_data\n",
    "            df_w_prints = self.extract_event_data(data_frames['prints'])\n",
    "            df_w_taps = self.extract_event_data(data_frames['taps'])\n",
    "            df_w_pays = data_frames['pays'].copy()\n",
    "\n",
    "            # Add 'week' column indicating the week of the month\n",
    "            df_w_prints['week'] = df_w_prints['day'].dt.isocalendar().week\n",
    "\n",
    "            # Creating a 'day+value_prop' identifier for merging\n",
    "            df_w_prints['day_value_prop'] = df_w_prints['day'].astype(str) + df_w_prints['value_prop']\n",
    "            df_w_taps['day_value_prop'] = df_w_taps['day'].astype(str) + df_w_taps['value_prop']\n",
    "\n",
    "            # Merge to find if prints were clicked\n",
    "            merged_df = pd.merge(df_w_prints, df_w_taps, \n",
    "                                 on=['user_id', 'day'], how='left', suffixes=('_prints', '_taps'))\n",
    "            merged_df['clicked'] = merged_df['day_value_prop_prints'] == merged_df['day_value_prop_taps']\n",
    "            merged_df['clicked'] = merged_df['clicked'].astype(int)  # Convert boolean to 1 or 0\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task_2\n",
    "Above is a summary about the [**task_2_views_on_each_value_prop_last_3_weeks**](../data/gold/task_2_views_on_each_value_prop_last_3_weeks.csv) csv file output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>week_prints</th>\n",
       "      <th>value_prop_prints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>last-3</td>\n",
       "      <td>{'link_cobro': 2, 'prepaid': 1, 'credits_consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>last-3</td>\n",
       "      <td>{'cellphone_recharge': 1, 'point': 1, 'send_mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>last-3</td>\n",
       "      <td>{'transport': 1, 'point': 1, 'cellphone_rechar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>last-3</td>\n",
       "      <td>{'cellphone_recharge': 1, 'credits_consumer': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>last-3</td>\n",
       "      <td>{'transport': 1, 'point': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id week_prints                                  value_prop_prints\n",
       "0        1      last-3  {'link_cobro': 2, 'prepaid': 1, 'credits_consu...\n",
       "1        2      last-3  {'cellphone_recharge': 1, 'point': 1, 'send_mo...\n",
       "2        3      last-3  {'transport': 1, 'point': 1, 'cellphone_rechar...\n",
       "3        4      last-3  {'cellphone_recharge': 1, 'credits_consumer': ...\n",
       "4        5      last-3                       {'transport': 1, 'point': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we filtered last-3 weeks (the last 3 weeks before the last week of the month)\n",
    "df_task2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000    1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "99949     1\n",
      "         ..\n",
      "8         1\n",
      "9         1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "Name: user_id, Length: 76804, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# As we could see above, there is 1 observation for each user_id, as expected.\n",
    "print(df_task2.user_id.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the user_id:1\n",
      "{'link_cobro': 2, 'prepaid': 1, 'credits_consumer': 1, 'transport': 2, 'point': 1, 'send_money': 1, 'cellphone_recharge': 1}\n",
      "\n",
      "for the user_id:2\n",
      "{'cellphone_recharge': 1, 'point': 1, 'send_money': 1}\n",
      "\n",
      "for the user_id:3\n",
      "{'transport': 1, 'point': 1, 'cellphone_recharge': 1, 'link_cobro': 1, 'prepaid': 1, 'send_money': 1}\n",
      "\n",
      "for the user_id:4\n",
      "{'cellphone_recharge': 1, 'credits_consumer': 1, 'prepaid': 1, 'link_cobro': 1}\n",
      "\n",
      "for the user_id:5\n",
      "{'transport': 1, 'point': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#So for the last-3 weeks, we have this distribuction of value_prop_prints that appeard for the users:\n",
    "\n",
    "for ix,row in df_task2.head().iterrows():\n",
    "    print(f\"for the user_id:{row.user_id}\\n{row.value_prop_prints}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we also used all the SilverDataProcessor pipeline and on the merged data we applyied some agg with groupby:\n",
    "\n",
    "plus: I used the [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter) function that apply a optimized implementation of counts algoritimics\n",
    "\n",
    "```python\n",
    "            # ○ task2: Each of the value props views number in the last 3 weeks prior to the print mentioned before.\n",
    "            f_counter = lambda x: dict(Counter(list(x)))\n",
    "            df_agg_value_prop_views = data.groupby(['user_id', 'week_prints']).agg({\"value_prop_prints\": f_counter}).reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task_3\n",
    "Above is a summary about the [**task_3_clickes_on_each_value_prop_last_3_weeks**](../data/gold/task_3_clickes_on_each_value_prop_last_3_weeks.csv) csv file output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'point': 0, 'cellphone_recharge': 0, 'credits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{'point': 0, 'cellphone_recharge': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{'point': 0, 'cellphone_recharge': 1, 'credits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>{'point': 1, 'cellphone_recharge': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{'point': 0, 'cellphone_recharge': 0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            clicked\n",
       "0        1  {'point': 0, 'cellphone_recharge': 0, 'credits...\n",
       "1        2              {'point': 0, 'cellphone_recharge': 0}\n",
       "2        3  {'point': 0, 'cellphone_recharge': 1, 'credits...\n",
       "3        4              {'point': 1, 'cellphone_recharge': 0}\n",
       "4        5              {'point': 0, 'cellphone_recharge': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we filtered last-3 weeks (the last 3 weeks before the last week of the month)\n",
    "df_task3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000    1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "99949     1\n",
      "         ..\n",
      "8         1\n",
      "9         1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "Name: user_id, Length: 76804, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# As we could see above, there is 1 observation for each user_id, as expected.\n",
    "print(df_task3.user_id.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the user_id:1\n",
      "{'point': 0, 'cellphone_recharge': 0, 'credits_consumer': 0, 'link_cobro': 0, 'transport': 0}\n",
      "\n",
      "for the user_id:2\n",
      "{'point': 0, 'cellphone_recharge': 0}\n",
      "\n",
      "for the user_id:3\n",
      "{'point': 0, 'cellphone_recharge': 1, 'credits_consumer': 0}\n",
      "\n",
      "for the user_id:4\n",
      "{'point': 1, 'cellphone_recharge': 0}\n",
      "\n",
      "for the user_id:5\n",
      "{'point': 0, 'cellphone_recharge': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#So for the last-3 weeks, we have this distribuction of\n",
    "# clicked over each print\n",
    "\n",
    "for ix,row in df_task3.head().iterrows():\n",
    "    print(f\"for the user_id:{row.user_id}\\n{row.clicked}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 3 we did a very simmilalr approach:\n",
    "\n",
    "- used transformed data from Silver layer;\n",
    "- apply agg and groupby function:\n",
    "\n",
    "  ```python\n",
    "            # ○ task3: Number of times a user clicked on each of the value props in the last 3 weeks prior to the print mentioned before\n",
    "            msk = data.week_prints == 'last'\n",
    "            df_agg_clicks_for_each_value_prop = self._agg_clicks_data_for_each_user(data[~msk])\n",
    "\n",
    "            # The auxiliar function that we used on this case:\n",
    "                def _agg_clicks_data_for_each_user(self, df):\n",
    "\n",
    "                    # Aggregate the data\n",
    "                    result_df = df.groupby('user_id').agg({\n",
    "                        'clicked': lambda x: dict(zip(df['value_prop_prints'], x))\n",
    "                        }).reset_index()\n",
    "\n",
    "                    return result_df\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task_4 and task_5:\n",
    "Above is a summary about the [**tasks_4_and_5_user_payments_summary_last_3_weeks**](../data/gold/tasks_4_and_5_user_payments_summary_last_3_weeks.csv) csv file output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>count_of_pays_for_each_value_props</th>\n",
       "      <th>sum_of_pays_for_each_value_props</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>{'credits_consumer': 1, 'point': 1}</td>\n",
       "      <td>{'credits_consumer': 17.23, 'point': 53.15}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>{'credits_consumer': 2}</td>\n",
       "      <td>{'credits_consumer': 90.50999999999999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>{'credits_consumer': 1, 'point': 1}</td>\n",
       "      <td>{'credits_consumer': 171.96, 'point': 7.39}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>{'credits_consumer': 1}</td>\n",
       "      <td>{'credits_consumer': 4.66}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>{'credits_consumer': 1}</td>\n",
       "      <td>{'credits_consumer': 101.38}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   count_of_pays_for_each_value_props  \\\n",
       "0       12  {'credits_consumer': 1, 'point': 1}   \n",
       "1       15              {'credits_consumer': 2}   \n",
       "2       30  {'credits_consumer': 1, 'point': 1}   \n",
       "3       33              {'credits_consumer': 1}   \n",
       "4       44              {'credits_consumer': 1}   \n",
       "\n",
       "              sum_of_pays_for_each_value_props  \n",
       "0  {'credits_consumer': 17.23, 'point': 53.15}  \n",
       "1      {'credits_consumer': 90.50999999999999}  \n",
       "2  {'credits_consumer': 171.96, 'point': 7.39}  \n",
       "3                   {'credits_consumer': 4.66}  \n",
       "4                 {'credits_consumer': 101.38}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we filtered last-3 weeks (the last 3 weeks before the last week of the month)\n",
    "df_task4_and_task5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99993    1\n",
      "12       1\n",
      "15       1\n",
      "30       1\n",
      "33       1\n",
      "        ..\n",
      "83       1\n",
      "84       1\n",
      "91       1\n",
      "100      1\n",
      "101      1\n",
      "Name: user_id, Length: 21413, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# As we could see above, there is 1 observation for each user_id, as expected.\n",
    "print(df_task4_and_task5.user_id.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the user_id:12\n",
      "{'credits_consumer': 1, 'point': 1}\n",
      "{'credits_consumer': 17.23, 'point': 53.15}\n",
      "\n",
      "\n",
      "for the user_id:15\n",
      "{'credits_consumer': 2}\n",
      "{'credits_consumer': 90.50999999999999}\n",
      "\n",
      "\n",
      "for the user_id:30\n",
      "{'credits_consumer': 1, 'point': 1}\n",
      "{'credits_consumer': 171.96, 'point': 7.39}\n",
      "\n",
      "\n",
      "for the user_id:33\n",
      "{'credits_consumer': 1}\n",
      "{'credits_consumer': 4.66}\n",
      "\n",
      "\n",
      "for the user_id:44\n",
      "{'credits_consumer': 1}\n",
      "{'credits_consumer': 101.38}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#So for the last-3 weeks, we have this distribuction of value_prop_prints that appeard for the users:\n",
    "\n",
    "for ix,row in df_task4_and_task5.head().iterrows():\n",
    "    print(f\"for the user_id:{row.user_id}\\n{row.count_of_pays_for_each_value_props}\\n{row.sum_of_pays_for_each_value_props}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That task was more intersting because we needed to do some more groupby's before we apply the agg with the custom function:\n",
    "\n",
    "1. we needed to creat count_pays and sum_pays for each week on the last-3 weeks grouped by 3 keys:\n",
    "- `user_id`, `week_prints`, `value_prop_pays` -> because we want to calculate the sum for each week for each user for each value_prop on the value_prop_pays (value_prop from pays.csv data after transformation and merging)\n",
    "\n",
    "3. after that, we created a last\n",
    "```python\n",
    "            df_tmp_count_and_sum_pays = data.groupby(['user_id', 'week_prints', 'value_prop_pays']).agg({\"total\": set})\n",
    "            df_tmp_count_and_sum_pays[\"count_pays\"] = df_tmp_count_and_sum_pays.total.apply(lambda x: len(x))\n",
    "            df_tmp_count_and_sum_pays[\"sum_pays\"] = df_tmp_count_and_sum_pays.total.apply(lambda x: sum(x))\n",
    "            df_tmp_count_and_sum_pays = df_tmp_count_and_sum_pays.reset_index()\n",
    "\n",
    "            # Just w-3 data\n",
    "            msk_last_week = df_tmp_count_and_sum_pays.week_prints == \"last\"\n",
    "            df_tmp_count_and_sum_pays = df_tmp_count_and_sum_pays[~msk_last_week]\n",
    "\n",
    "            # agg last 3 weeks data\n",
    "            df_agg_count_and_sum_pays = self._agg_pays_data_for_each_user(df_tmp_count_and_sum_pays)\n",
    "\n",
    "            df_agg_count_and_sum_pays.rename(columns={\"count_pays\":\"count_of_pays_for_each_value_props\",\"sum_pays\":\"sum_of_pays_for_each_value_props\"}, inplace=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
